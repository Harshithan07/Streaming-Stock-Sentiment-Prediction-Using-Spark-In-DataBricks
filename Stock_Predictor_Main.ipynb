{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "927abd35-6323-4148-baf2-1637e717f7f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Real-Time Stock Sentiment Analysis\n",
    "\n",
    "Dataset Link for training the model: https://huggingface.co/datasets/TimKoornstra/financial-tweets-sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3f8f52b3-76a8-42c5-96ff-13a99a9ee885",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## A. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1cda0601-0d1c-4ec2-b5c5-e26a39fb6a56",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Install hugging face datasets\n",
    "\n",
    "using  `%pip install datasets`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0760b28e-de71-4098-a5c3-adafafa747a9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting datasets\n  Using cached datasets-3.5.0-py3-none-any.whl (491 kB)\nCollecting huggingface-hub>=0.24.0\n  Using cached huggingface_hub-0.30.1-py3-none-any.whl (481 kB)\nCollecting multiprocess<0.70.17\n  Using cached multiprocess-0.70.16-py39-none-any.whl (133 kB)\nCollecting xxhash\n  Using cached xxhash-3.5.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (193 kB)\nRequirement already satisfied: packaging in /databricks/python3/lib/python3.9/site-packages (from datasets) (21.3)\nRequirement already satisfied: pandas in /databricks/python3/lib/python3.9/site-packages (from datasets) (1.4.2)\nRequirement already satisfied: numpy>=1.17 in /databricks/python3/lib/python3.9/site-packages (from datasets) (1.21.5)\nCollecting aiohttp\n  Using cached aiohttp-3.11.16-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.6 MB)\nCollecting pyarrow>=15.0.0\n  Using cached pyarrow-19.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (42.1 MB)\nRequirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from datasets) (3.9.0)\nCollecting dill<0.3.9,>=0.3.0\n  Using cached dill-0.3.8-py3-none-any.whl (116 kB)\nCollecting requests>=2.32.2\n  Using cached requests-2.32.3-py3-none-any.whl (64 kB)\nCollecting tqdm>=4.66.3\n  Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\nCollecting pyyaml>=5.1\n  Using cached PyYAML-6.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (737 kB)\nCollecting fsspec[http]<=2024.12.0,>=2023.1.0\n  Using cached fsspec-2024.12.0-py3-none-any.whl (183 kB)\nCollecting frozenlist>=1.1.1\n  Using cached frozenlist-1.5.0-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\nCollecting multidict<7.0,>=4.5\n  Using cached multidict-6.3.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (231 kB)\nCollecting aiosignal>=1.1.2\n  Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\nRequirement already satisfied: attrs>=17.3.0 in /databricks/python3/lib/python3.9/site-packages (from aiohttp->datasets) (21.4.0)\nCollecting yarl<2.0,>=1.17.0\n  Using cached yarl-1.18.3-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (321 kB)\nCollecting async-timeout<6.0,>=4.0\n  Using cached async_timeout-5.0.1-py3-none-any.whl (6.2 kB)\nCollecting aiohappyeyeballs>=2.3.0\n  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\nCollecting propcache>=0.2.0\n  Using cached propcache-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (209 kB)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /databricks/python3/lib/python3.9/site-packages (from huggingface-hub>=0.24.0->datasets) (4.1.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /databricks/python3/lib/python3.9/site-packages (from packaging->datasets) (3.0.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (1.26.9)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2021.10.8)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (3.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.32.2->datasets) (2.0.4)\nRequirement already satisfied: python-dateutil>=2.8.1 in /databricks/python3/lib/python3.9/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /databricks/python3/lib/python3.9/site-packages (from pandas->datasets) (2021.3)\nRequirement already satisfied: six>=1.5 in /databricks/python3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\nInstalling collected packages: propcache, multidict, frozenlist, yarl, async-timeout, aiosignal, aiohappyeyeballs, tqdm, requests, pyyaml, fsspec, dill, aiohttp, xxhash, pyarrow, multiprocess, huggingface-hub, datasets\n  Attempting uninstall: requests\n    Found existing installation: requests 2.27.1\n    Not uninstalling requests at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dd720d60-d621-4956-aa79-87e456418663\n    Can't uninstall 'requests'. No files were found to uninstall.\n  Attempting uninstall: pyarrow\n    Found existing installation: pyarrow 7.0.0\n    Not uninstalling pyarrow at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-dd720d60-d621-4956-aa79-87e456418663\n    Can't uninstall 'pyarrow'. No files were found to uninstall.\nSuccessfully installed aiohappyeyeballs-2.6.1 aiohttp-3.11.16 aiosignal-1.3.2 async-timeout-5.0.1 datasets-3.5.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 huggingface-hub-0.30.1 multidict-6.3.2 multiprocess-0.70.16 propcache-0.3.1 pyarrow-19.0.1 pyyaml-6.0.2 requests-2.32.3 tqdm-4.67.1 xxhash-3.5.0 yarl-1.18.3\nPython interpreter will be restarted.\n"
     ]
    }
   ],
   "source": [
    "%pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76f24520-1d31-4e3d-92c2-bff53382fe9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary PySpark functions\n",
    "from pyspark.sql.functions import col, length, current_timestamp\n",
    "\n",
    "# Import feature engineering APIs: Tokenizer, StopWordsRemover, SQLTransformer\n",
    "from pyspark.ml.feature import Tokenizer, StopWordsRemover, SQLTransformer\n",
    "\n",
    "# Import feature engineering APIs for text to vector conversion: Word2Vec, HashingTF, IDF, CountVectorizer\n",
    "from pyspark.ml.feature import HashingTF, IDF, CountVectorizer, Word2Vec\n",
    "\n",
    "# Import the following multi-class classifier: RandomForestClassifier, LogisticRegression, DecisionTreeClassifier\n",
    "from pyspark.ml.classification import RandomForestClassifier, LogisticRegression, DecisionTreeClassifier\n",
    "\n",
    "# Import the Evaluator for F1 and Accuracy\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "# import Pipeline and PipelineModel\n",
    "from pyspark.ml import Pipeline, PipelineModel\n",
    "\n",
    "# import other python libraries, including `json`, `time`\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Import `load_dataset` from `datasets`\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f29e0bf3-3f76-478c-8087-f3d34d05a560",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Load data into PySpark DataFrame\n",
    "\n",
    "Run the following to download the dataset\n",
    "\n",
    "```\n",
    "dataset = load_dataset(\"TimKoornstra/financial-tweets-sentiment\")\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d59d9aef-3618-4481-b6e7-c32127ad872a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n |-- sentiment: long (nullable = true)\n |-- tweet: string (nullable = true)\n |-- url: string (nullable = true)\n\n+---------+--------------------+--------------------+\n|sentiment|               tweet|                 url|\n+---------+--------------------+--------------------+\n|        2|$BYND - JPMorgan ...|https://huggingfa...|\n|        2|$CCL $RCL - Nomur...|https://huggingfa...|\n|        2|$CX - Cemex cut a...|https://huggingfa...|\n|        2|$ESS: BTIG Resear...|https://huggingfa...|\n|        2|$FNKO - Funko sli...|https://huggingfa...|\n|        2|$FTI - TechnipFMC...|https://huggingfa...|\n|        2|$GM - GM loses a ...|https://huggingfa...|\n|        2|$GM: Deutsche Ban...|https://huggingfa...|\n|        2|$GTT: Cowen cuts ...|https://huggingfa...|\n|        2|$HNHAF $HNHPD $AA...|https://huggingfa...|\n+---------+--------------------+--------------------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"TimKoornstra/financial-tweets-sentiment\")\n",
    "train_data = dataset['train']\n",
    "df = spark.createDataFrame(train_data)\n",
    "df.cache()\n",
    "df.printSchema()\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a6163227-13ec-4610-bb4d-af5bd5759091",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Load the dataset['train'] into a Spark DataFrame `spark_df`\n",
    "- cache the spark dataframe for faster processing later\n",
    "- Display the schema and first 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "faaadd77-cc52-4bd2-93c7-cc7d2e9e001b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Check the distribution of sentiment values\n",
    "\n",
    "Query DataFrame to show the counts of different sentiment values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dbcf5ad-98cf-4980-97c4-9d742f1a92cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-----+\n|sentiment|count|\n+---------+-----+\n|        0|12181|\n|        1|17368|\n|        2| 8542|\n+---------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"sentiment\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "de734c3c-092f-48ed-8b86-d3dd47b272b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## B. Build the Feature Engineering Pipeline\n",
    "\n",
    "In the following, we will carry out feature engineering steps that will be become stages of a pipeline. The goal is to build a feature vector that is most suitable for predict the sentiment of the tweet.\n",
    "\n",
    "To prepare the text data for training:\n",
    "- Augment data with timestamp\n",
    "- Cleaning tweet texts, remove identifiers, URLs, etc.\n",
    "- Tokenize the tweet text.\n",
    "- Remove stop words from the tokenized text.\n",
    "- Convert the list of words into numerical feature vectors using Word2Vec or TF-IDF.\n",
    "\n",
    "\n",
    "I ask you to \n",
    "- test Transformer/Estimators as you go, by displaying, say, the first 30 rows.\n",
    "-  But keep in mind, for them to work together, input columns must already exist in the dataframe, which could be generated in the previous step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "aeb0a4ea-c94f-4e4f-95df-79415e984b3d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. augment original data \n",
    "\n",
    "augment the dataframe by adding `cleaned_tweet` and `tweet_time` columns, where \n",
    "\n",
    "- `cleaned_tweet` is a copy of tweet for now (we will change this in later steps)\n",
    "- `tweet_time` is calculated using `current_timestamp`\n",
    "\n",
    "> Hints: Using `SQLTransformer`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a4063bf1-aa2e-4d0d-b309-1da438d428b6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+-----------------------+\n|sentiment|tweet                                                                                                 |url                                                                      |cleaned_tweet                                                                                         |tweet_time             |\n+---------+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+-----------------------+\n|2        |$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT                         |https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment|$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT                         |2025-04-05 23:58:25.483|\n|2        |$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3|https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment|$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3|2025-04-05 23:58:25.483|\n|2        |$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb        |https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment|$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb        |2025-04-05 23:58:25.483|\n|2        |$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N                                           |https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment|$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N                                           |2025-04-05 23:58:25.483|\n|2        |$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB                               |https://huggingface.co/datasets/zeroshot/twitter-financial-news-sentiment|$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB                               |2025-04-05 23:58:25.483|\n+---------+------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------+-----------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp\n",
    "\n",
    "augment = SQLTransformer(\n",
    "    statement=\"SELECT *, tweet as cleaned_tweet, current_timestamp() as tweet_time FROM __THIS__\"\n",
    ")\n",
    "df_augmented = augment.transform(df)\n",
    "df_augmented.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c42b5b51-8ca7-425e-8563-3cebb3e79b6e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 2.  Text cleaning\n",
    "\n",
    "I have written a few additional text cleaning steps. They mostly use regular expressions to remove/replace unwanted segments\n",
    "\n",
    "> Hints: Nowadays, you can ask GPT to write these regular expressions for you, and you can test them. \n",
    "\n",
    "- Each transformer replace `cleaned_tweet` with a transformed one.\n",
    "- Your job is to choose that ones that make most sense to you (keep in mind, your job is to use these words to predict sentiment)\n",
    "- Sequence matters, e.g. you want to remove ALL CAP words before converting them to lower case. \n",
    "- Test your favoriate steps with a pipeline  `cleaning_pipeline` instead of each one individually. \n",
    "- feel free to come up new ones that make sense to you.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f42c9aad-05f7-42f5-a0ed-16a2e8668a2d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "st2 = SQLTransformer(statement=\"select tweet_time, tweet, REGEXP_REPLACE(cleaned_tweet, r'https?://\\S+', '') as cleaned_tweet, sentiment from __THIS__\") #remove URLs https:// and http:// \\S means a non-space character.\n",
    "st3 = SQLTransformer(statement=\"select tweet_time, tweet, REGEXP_REPLACE(cleaned_tweet, r'[^\\w\\s]','') as cleaned_tweet, sentiment from __THIS__\") # remove non-alphanumeric, non space characters [^\\w\\s]  \n",
    "st4 = SQLTransformer(statement=\"select tweet_time, tweet, REGEXP_REPLACE(cleaned_tweet, r'\\\\b[A-Z]*\\\\b','') as cleaned_tweet, sentiment from __THIS__\")  #remove all cap words,  [A-Z] is a cap character, \\b marks word boundary \n",
    "st5 = SQLTransformer(statement=\"select tweet_time, tweet, REGEXP_REPLACE(cleaned_tweet, r'\\\\b\\w\\\\b',' ') as cleaned_tweet, sentiment from __THIS__\") # remove word that is exactly one-character long\n",
    "st6 = SQLTransformer(statement=\"select tweet_time, tweet, REGEXP_REPLACE(cleaned_tweet, r'\\\\b\\d+\\\\b','') as cleaned_tweet, sentiment from __THIS__\") # remove all digits words, \\d is a digit\n",
    "st7 = SQLTransformer(statement=\"select tweet_time, tweet, REGEXP_REPLACE(cleaned_tweet, r'\\s+',' ') as cleaned_tweet, sentiment from __THIS__\") # repace consecutive space with one, \\s is a space or a tab.\n",
    "st8 = SQLTransformer(statement=\"select tweet_time, tweet, trim(lower(cleaned_tweet)) as cleaned_tweet, sentiment from __THIS__\") # convert to lower case and remove trailing/leading spaces. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "942817e3-897d-4f48-a48f-936472b0717c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------+------------------------------------------------------------------+\n|tweet                                                                                                 |cleaned_tweet                                                     |\n+------------------------------------------------------------------------------------------------------+------------------------------------------------------------------+\n|$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT                         |jpmorgan reels in expectations on beyond meat                     |\n|$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3|nomura points to bookings weakness at carnival and royal caribbean|\n|$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb        |cemex cut at credit suisse morgan on weak building outlook        |\n|$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N                                           |research cuts to neutral                                          |\n|$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB                               |funko slides after piper jaffray cut                              |\n+------------------------------------------------------------------------------------------------------+------------------------------------------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "cleaning_pipeline = Pipeline(stages=[augment, st1, st2, st3, st4, st5, st6, st7, st8])\n",
    "df_cleaned = cleaning_pipeline.fit(df).transform(df)\n",
    "df_cleaned.select(\"tweet\", \"cleaned_tweet\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5edbb3a6-42d5-4a06-9d41-7802069abf22",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Tokenization\n",
    "\n",
    "that is, to convert a sentence into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "192cbd18-78d2-4a88-8285-752171dcde06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------+-----------------------------------------------------------------------------+\n|cleaned_tweet                                                     |words                                                                        |\n+------------------------------------------------------------------+-----------------------------------------------------------------------------+\n|jpmorgan reels in expectations on beyond meat                     |[jpmorgan, reels, in, expectations, on, beyond, meat]                        |\n|nomura points to bookings weakness at carnival and royal caribbean|[nomura, points, to, bookings, weakness, at, carnival, and, royal, caribbean]|\n|cemex cut at credit suisse morgan on weak building outlook        |[cemex, cut, at, credit, suisse, morgan, on, weak, building, outlook]        |\n|research cuts to neutral                                          |[research, cuts, to, neutral]                                                |\n|funko slides after piper jaffray cut                              |[funko, slides, after, piper, jaffray, cut]                                  |\n+------------------------------------------------------------------+-----------------------------------------------------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"cleaned_tweet\", outputCol=\"words\")\n",
    "df_tokenized = tokenizer.transform(df_cleaned)\n",
    "df_tokenized.select(\"cleaned_tweet\", \"words\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "18e6e142-caa8-4ea7-a796-18f85493f6c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Remove stop words\n",
    "\n",
    "A standard StopWordsRemover is defined using:\n",
    "```python\n",
    "StopWordsRemover(inputCol=, outputCol=)\n",
    "```\n",
    "It will use a standard stop word list.\n",
    "\n",
    "If you want to modify/augment the list of stop words:\n",
    "\n",
    "```python\n",
    "# Get the default stopword list\n",
    "stopwords_list = StopWordsRemover().getStopWords()\n",
    "# modify or augment it \n",
    "stopwords_list_enhanced = ...\n",
    "# build a remover with custom word list\n",
    "remover = StopWordsRemover(inputCol=, outputCol=, stopWords=stopwords_list_enhanced)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ed915211-1c84-4ab3-a9b7-eb0503cfaf4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------------------------+\n|filtered_words                                                  |\n+----------------------------------------------------------------+\n|[jpmorgan, reels, expectations, beyond, meat]                   |\n|[nomura, points, bookings, weakness, carnival, royal, caribbean]|\n|[cemex, cut, credit, suisse, morgan, weak, building, outlook]   |\n|[research, cuts, neutral]                                       |\n|[funko, slides, piper, jaffray, cut]                            |\n+----------------------------------------------------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "remover = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered_words\")\n",
    "df_filtered = remover.transform(df_tokenized)\n",
    "df_filtered.select(\"filtered_words\").show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3805505f-83c6-4c1c-a0d6-1bdf3734d517",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5. Vectorization text\n",
    "\n",
    "You can convert list of words into numercal vectors in a few different ways.\n",
    "\n",
    "- `Word2Vec`: neural network based word embedding developed by Google in 2013\n",
    "  - `Word2Vec([vectorSize=], [minCount=], inputCol=, outputCol=, ....)`\n",
    "  - `[fittedModel].getVectors()`: will return a list of keyword and associated vectors\n",
    "- `CountVectorizer`: a simple way to vectorize based on word count.\n",
    "  - `CountVectorizer([minTF=], [vocabSize=], inputCol=, outputCol=, ...)` \n",
    "- `TF-IDF`: based on term frequency, but weighted by inverse document frequence (penalize words that appear too often, like stop words). In pySpark, this is done in two steps via HashTF and IDF\n",
    "  -  `HashingTF(inputCol=, outputCol=, [numFeatures=])`: the last one is how many terms/words\n",
    "  -  `IDF(inputCol=, outputCol=)`: weigh term frequency by IDF\n",
    "\n",
    "Implement one of the approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "978bca04-6848-48c8-aa8b-341882ce7cde",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n|            features|\n+--------------------+\n|(1000,[336,408,82...|\n|(1000,[42,166,213...|\n|(1000,[206,348,50...|\n|(1000,[78,432,437...|\n|(1000,[206,346,37...|\n+--------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "hashingTF = HashingTF(inputCol=\"filtered_words\", outputCol=\"raw_features\", numFeatures=1000)\n",
    "idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "\n",
    "df_featurized = hashingTF.transform(df_filtered)\n",
    "df_tfidf = idf.fit(df_featurized).transform(df_featurized)\n",
    "df_tfidf.select(\"features\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fe6e29f2-1c70-4c03-9a14-f664a9e9bab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## C. Prepare for Classifier training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ce92945e-311a-4a0a-ab7f-5a1ed68bdd9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 1. Split the prepared dataframe\n",
    "\n",
    "- into training/testing (`train_df`, `test_df`) using 80-20 random split. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "718eccc6-c458-4de0-86a1-2bb1b78e100e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "train_df, test_df = df_tfidf.randomSplit([0.8, 0.2], seed=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "52783ead-e477-4072-9239-12ab12dd26c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "\n",
    "### 2. Train a Model(s)\n",
    "\n",
    "Now, we'll train machine learning models on the processed dataset and evaluate their performance.\n",
    "\n",
    "Because the sentiment has three values,  **(1 for bullish, 2 for bearish, and 0 for neutral)**, we need to only use Classifier that works with multiple classes. \n",
    "\n",
    "You may try:\n",
    "\n",
    "- Random Forest\n",
    "- Logist Regression:  by default, it can automatically choose between binomial (two classes)/multinomial (multiple classes)\n",
    "- Decision Tree\n",
    "\n",
    "make sure you save the fitted model as a variable, which may use in the Pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "644d55ad-590b-4e01-b205-0e5ff02e8afa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"sentiment\")\n",
    "lr_model = lr.fit(train_df)\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestClassifier(featuresCol=\"features\", labelCol=\"sentiment\")\n",
    "rf_model = rf.fit(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac6c5f4c-aa99-49bf-8628-08dd25dbc39f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier, RandomForestClassifier, LogisticRegression\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "\n",
    "evaluator_f1 = MulticlassClassificationEvaluator(labelCol=\"sentiment\", metricName=\"f1\")\n",
    "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"sentiment\", metricName=\"accuracy\")\n",
    "\n",
    "dt = DecisionTreeClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"sentiment\",\n",
    "    maxDepth=15, \n",
    "    minInstancesPerNode=20,  \n",
    "    impurity='gini'  \n",
    ")\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"sentiment\",\n",
    "    numTrees=100, \n",
    "    maxDepth=12,\n",
    "    featureSubsetStrategy='sqrt', \n",
    "    subsamplingRate=0.8\n",
    ")\n",
    "\n",
    "lr = LogisticRegression(\n",
    "    featuresCol=\"features\",\n",
    "    labelCol=\"sentiment\",\n",
    "    maxIter=20, \n",
    "    regParam=0.1, \n",
    "    elasticNetParam=0.5, \n",
    "    family='multinomial'\n",
    ")\n",
    "\n",
    "paramGrid = ParamGridBuilder() \\\n",
    "    .addGrid(lr.regParam, [0.01, 0.1, 0.3]) \\\n",
    "    .addGrid(lr.maxIter, [10, 20]) \\\n",
    "    .build()\n",
    "\n",
    "crossval = CrossValidator(\n",
    "    estimator=lr,\n",
    "    estimatorParamMaps=paramGrid,\n",
    "    evaluator=evaluator_f1,\n",
    "    numFolds=3\n",
    ")\n",
    "\n",
    "dt_model = dt.fit(train_df)\n",
    "rf_model = rf.fit(train_df)\n",
    "lr_model = crossval.fit(train_df).bestModel  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1a9bcfa-0760-47f5-bdc4-7565993637aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 3. Model evaluation\n",
    "\n",
    "define an F1 evalutor and an accuracy evaluator.\n",
    "\n",
    "- use the train model to make a prediction on the `test_df` dataset.\n",
    "- Evaluate/print its F1-score and accuracy score.\n",
    "- try to obtain an >0.5 F1-score and a >0.5 accuracy (>0.6 is possiable but not required)\n",
    "    - try differnet feature engineering steps/models or their parameters and see if it leads to better result.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a7ae0b8-f4c0-4fe2-92d6-8d86629e7218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\nDecision Tree Performance:\n- F1 Score: 0.423\n- Accuracy: 0.507\n+----------+-----+\n|prediction|count|\n+----------+-----+\n|       0.0|  452|\n|       1.0| 6544|\n|       2.0|  482|\n+----------+-----+\n\n\nRandom Forest Performance:\n- F1 Score: 0.393\n- Accuracy: 0.505\n+----------+-----+\n|prediction|count|\n+----------+-----+\n|       0.0|  509|\n|       1.0| 6915|\n|       2.0|   54|\n+----------+-----+\n\n\nLogistic Regression Performance:\n- F1 Score: 0.552\n- Accuracy: 0.579\n+----------+-----+\n|prediction|count|\n+----------+-----+\n|       0.0| 1696|\n|       1.0| 5059|\n|       2.0|  723|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, name):\n",
    "    predictions = model.transform(test_df)\n",
    "    f1 = evaluator_f1.evaluate(predictions)\n",
    "    acc = evaluator_acc.evaluate(predictions)\n",
    "    print(f\"\\n{name} Performance:\")\n",
    "    print(f\"- F1 Score: {f1:.3f}\")\n",
    "    print(f\"- Accuracy: {acc:.3f}\")\n",
    "    predictions.groupBy(\"prediction\").count().show()\n",
    "\n",
    "evaluate_model(dt_model, \"Decision Tree\")\n",
    "evaluate_model(rf_model, \"Random Forest\")\n",
    "evaluate_model(lr_model, \"Logistic Regression\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "85eec0b8-d643-4d4b-acd9-ca43a9e379fb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Logistic Regression** performs better than other models and give >0.5 in f1 and accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d851d824-4101-4a09-8a93-d55a1fda7912",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. inpect predictions\n",
    "\n",
    "- select `tweet`, `sentiment`, and `predition` to compare them. \n",
    "- show the distribution of `sentment`\n",
    "- compare it withe the distribution of `prediction`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "83dbece0-2cd5-43a2-9841-93945b8cb7d3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+----------+\n|               tweet|sentiment|prediction|\n+--------------------+---------+----------+\n|\"Be nice to peopl...|        0|       1.0|\n|\"Frozen II\" is ex...|        0|       0.0|\n|\"I almost burst i...|        0|       1.0|\n|\"In my time at th...|        0|       1.0|\n|\"No water. No wat...|        0|       0.0|\n|\"The Committee al...|        0|       2.0|\n|\"They All Knew!\" ...|        0|       1.0|\n|\"We Totally Faile...|        2|       0.0|\n|#FridayReads: Wha...|        0|       2.0|\n|#HOLIDAY #SPECIAL...|        0|       0.0|\n+--------------------+---------+----------+\nonly showing top 10 rows\n\n+---------+-----+\n|sentiment|count|\n+---------+-----+\n|        0| 2362|\n|        1| 3415|\n|        2| 1701|\n+---------+-----+\n\n+----------+-----+\n|prediction|count|\n+----------+-----+\n|       0.0| 1727|\n|       1.0| 5181|\n|       2.0|  570|\n+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "predictions.select(\"tweet\", \"sentiment\", \"prediction\").show(10)\n",
    "predictions.groupBy(\"sentiment\").count().show()\n",
    "predictions.groupBy(\"prediction\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "54739e0c-f8d1-42f3-a9e3-3a63bc33290d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5. define, train, and deploy a pipeline\n",
    "\n",
    "- define a pipeline that has your chosen feature engineering steps and the model. \n",
    "- test the pipeline on the original data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5accaebf-8060-42f4-a660-ea1db9a69f53",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "full_pipeline = Pipeline(stages=[\n",
    "    augment,\n",
    "    st1, st2, st3, st4, st5, st6, st7, st8,\n",
    "    tokenizer,\n",
    "    remover,\n",
    "    hashingTF,       \n",
    "    idf,\n",
    "    lr               \n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "81c55a6b-d5e6-473d-b776-3becce8d78d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------------------------------------------------------------------+---------+----------+\n|tweet                                                                                                 |sentiment|prediction|\n+------------------------------------------------------------------------------------------------------+---------+----------+\n|$BYND - JPMorgan reels in expectations on Beyond Meat https://t.co/bd0xbFGjkT                         |2        |1.0       |\n|$CCL $RCL - Nomura points to bookings weakness at Carnival and Royal Caribbean https://t.co/yGjpT2ReD3|2        |1.0       |\n|$CX - Cemex cut at Credit Suisse, J.P. Morgan on weak building outlook https://t.co/KN1g4AWFIb        |2        |1.0       |\n|$ESS: BTIG Research cuts to Neutral https://t.co/MCyfTsXc2N                                           |2        |1.0       |\n|$FNKO - Funko slides after Piper Jaffray PT cut https://t.co/z37IJmCQzB                               |2        |1.0       |\n|$FTI - TechnipFMC downgraded at Berenberg but called Top Pick at Deutsche Bank https://t.co/XKcPDilIuU|2        |1.0       |\n|$GM - GM loses a bull https://t.co/tdUfG5HbXy                                                         |2        |1.0       |\n|$GM: Deutsche Bank cuts to Hold https://t.co/7Fv1ZiFZBS                                               |2        |1.0       |\n|$GTT: Cowen cuts to Market Perform                                                                    |2        |1.0       |\n|$HNHAF $HNHPD $AAPL - Trendforce cuts iPhone estimate after Foxconn delay https://t.co/rlnEwzlzzS     |2        |1.0       |\n+------------------------------------------------------------------------------------------------------+---------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "pipeline_model = full_pipeline.fit(df)\n",
    "transformed_df = pipeline_model.transform(df)\n",
    "transformed_df.select(\"tweet\", \"sentiment\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4bc63ca7-a893-478a-acc3-f36a11b0b398",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 6. Save the pipeline model\n",
    "\n",
    "- save to `/databricks/driver/tmp/sentiment_model` on the driver node's local file system using PipelineModel's `write().overwrite().save(path)` \n",
    "    - `.overwrite()` will set it to overwrite mode (if already exists)\n",
    "    - `.write()` returns an **MLWriter** instance\n",
    "- we later will use this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d55a8f57-44a5-4c57-9664-f749d50c69f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "pipeline_model.write().overwrite().save(\"/databricks/driver/tmp/sentiment_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "53305bda-3d82-4cfe-83f1-8b199b705769",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## D. Set up Streaming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96a88dea-a891-410a-a979-3f51493f0817",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 1. Develop the streaming processing logic in the batch mode\n",
    "Since we don't have **real-time tweets**, we will simulate streaming using **historical financial tweets** from the dataset.\n",
    "\n",
    "- Please first run the data generator step 2 for a while to put some files in the target folder on the driver node.\n",
    "- load the data into a Spark Dataframe `df` from `/databricks/driver/streaming_source` (in the local file system)\n",
    "- load the saved pipeline and apply it on the data frame.\n",
    "- creates a `counts_df` dataframe that counts the number of predicted sentiment values by value type (i.e., 0, 1, 2) and window (using 1 minute tumbling window)\n",
    "- validate the results by displaying them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88630eda-9eb4-425b-afb3-483694f01c54",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-------------------+\n|tweet                                                                                                                                                                                      |sentiment|tweet_time         |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-------------------+\n|Amazon Announces First-Ever ‘Amazon Future Engineer Teacher of the Year Awards,’ Awarding Seven All-Star Teachers $25,000 Prize Packages for Exemplar Work with Students Across the Country|0        |2025-04-06 00:15:01|\n|Samenvatting: Japanse NEDO en Panasonic behalen ‘s werelds hoogste conversie-efficiëntie van 16,09% voor grootste zonnecelmodule met Perovskite per gebied                                 |0        |2025-04-06 00:25:26|\n|Medical Properties Trust, Inc. Completes 2019 With Record $4.5 Billion in Acquisitions for 64% Growth Rate and Delivers Market-Leading Shareholder Returns                                 |1        |2025-04-06 00:11:13|\n|The stricken Bank of Jinzhou will unload $21 billion of assets to the central bank for less than a third of theirÃ‚Â r… https://t.co/xGuCP9lBlH                                            |0        |2025-04-06 00:06:25|\n|Some gamers accessing Google's new cloud gaming platform, Stadia, through a Chromecast Ultra dongle are reporting trouble… https://t.co/t6dCbMr0C3                                         |2        |2025-04-06 00:08:48|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+-------------------+\nonly showing top 5 rows\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"tweet\", StringType()) \\\n",
    "    .add(\"sentiment\", IntegerType()) \\\n",
    "    .add(\"tweet_time\", StringType())\n",
    "\n",
    "df = spark.read.schema(schema).json(\"file:/databricks/driver/streaming_source\")\n",
    "\n",
    "df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aabd17f0-2fae-46db-8c29-e383053da72d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "pipeline_model = PipelineModel.load(\"/databricks/driver/tmp/sentiment_model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b1d2c87-ebac-4730-ad96-7abd69fc13cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+----------+\n|tweet                                                                                                                                                                                      |sentiment|prediction|\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+----------+\n|Amazon Announces First-Ever ‘Amazon Future Engineer Teacher of the Year Awards,’ Awarding Seven All-Star Teachers $25,000 Prize Packages for Exemplar Work with Students Across the Country|0        |1.0       |\n|Samenvatting: Japanse NEDO en Panasonic behalen ‘s werelds hoogste conversie-efficiëntie van 16,09% voor grootste zonnecelmodule met Perovskite per gebied                                 |0        |1.0       |\n|Medical Properties Trust, Inc. Completes 2019 With Record $4.5 Billion in Acquisitions for 64% Growth Rate and Delivers Market-Leading Shareholder Returns                                 |1        |1.0       |\n|The stricken Bank of Jinzhou will unload $21 billion of assets to the central bank for less than a third of theirÃ‚Â r… https://t.co/xGuCP9lBlH                                            |0        |1.0       |\n|Some gamers accessing Google's new cloud gaming platform, Stadia, through a Chromecast Ultra dongle are reporting trouble… https://t.co/t6dCbMr0C3                                         |2        |1.0       |\n|“Vaping is taking us backward,” CVS Health CEO Larry Merlo says. “Something has to be done.” https://t.co/CwxbCtXvWW https://t.co/NBv10IejXx                                               |2        |1.0       |\n|“I think it’s a time of great transformation in retail right now.” @stitchfix CEO Katrina Lake discusses how the co… https://t.co/gQgzOBNaUg                                               |0        |1.0       |\n|Greenlane Renewables Signs New $7.0 Million System Supply Contract with the Renewable Natural Gas Company, a Leader in Landfill Gas to RNG Projects                                        |0        |1.0       |\n|Apple delaying the theatrical release of “The Banker” after one of the movie’s producers was accused of assault by… https://t.co/rqPyMuLZob                                                |0        |1.0       |\n|St. Louis Fed President Jim Bullard recommends declaring a  “National Pandemic Adjustment Period' and discusses thre… https://t.co/o942U7BKPI                                             |0        |1.0       |\n+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+---------+----------+\nonly showing top 10 rows\n\n"
     ]
    }
   ],
   "source": [
    "df_clean = df.drop(\"tweet_time\")\n",
    "predictions = pipeline_model.transform(df_clean)\n",
    "predictions.select(\"tweet\", \"sentiment\", \"prediction\").show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "249e9bf0-ed72-4482-9b04-45d96134ba4f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------------+----------+-----+\n|window                                    |prediction|count|\n+------------------------------------------+----------+-----+\n|{2025-04-06 00:48:00, 2025-04-06 00:49:00}|1.0       |2992 |\n+------------------------------------------+----------+-----+\n\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, window, to_timestamp\n",
    "\n",
    "predictions_ts = predictions.withColumn(\"tweet_time\", to_timestamp(\"tweet_time\"))\n",
    "\n",
    "counts_df = predictions_ts.groupBy(\n",
    "    window(col(\"tweet_time\"), \"1 minute\"),\n",
    "    col(\"prediction\")\n",
    ").count()\n",
    "\n",
    "counts_df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8b76d2ff-df50-4800-82d9-315c338df37c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>tweet_time</th><th>tweet</th><th>sentiment</th><th>prediction</th></tr></thead><tbody><tr><td>2025-04-06T00:55:21.678+0000</td><td>Amazon Announces First-Ever ‘Amazon Future Engineer Teacher of the Year Awards,’ Awarding Seven All-Star Teachers $25,000 Prize Packages for Exemplar Work with Students Across the Country</td><td>0</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>Samenvatting: Japanse NEDO en Panasonic behalen ‘s werelds hoogste conversie-efficiëntie van 16,09% voor grootste zonnecelmodule met Perovskite per gebied</td><td>0</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>Medical Properties Trust, Inc. Completes 2019 With Record $4.5 Billion in Acquisitions for 64% Growth Rate and Delivers Market-Leading Shareholder Returns</td><td>1</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>The stricken Bank of Jinzhou will unload $21 billion of assets to the central bank for less than a third of theirÃ‚Â r… https://t.co/xGuCP9lBlH</td><td>0</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>Some gamers accessing Google's new cloud gaming platform, Stadia, through a Chromecast Ultra dongle are reporting trouble… https://t.co/t6dCbMr0C3</td><td>2</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>“Vaping is taking us backward,” CVS Health CEO Larry Merlo says. “Something has to be done.” https://t.co/CwxbCtXvWW https://t.co/NBv10IejXx</td><td>2</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>“I think it’s a time of great transformation in retail right now.” @stitchfix CEO Katrina Lake discusses how the co… https://t.co/gQgzOBNaUg</td><td>0</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>Greenlane Renewables Signs New $7.0 Million System Supply Contract with the Renewable Natural Gas Company, a Leader in Landfill Gas to RNG Projects</td><td>0</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>Apple delaying the theatrical release of “The Banker” after one of the movie’s producers was accused of assault by… https://t.co/rqPyMuLZob</td><td>0</td><td>1.0</td></tr><tr><td>2025-04-06T00:55:21.678+0000</td><td>St. Louis Fed President Jim Bullard recommends declaring a  “National Pandemic Adjustment Period' and discusses thre… https://t.co/o942U7BKPI</td><td>0</td><td>1.0</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "2025-04-06T00:55:21.678+0000",
         "Amazon Announces First-Ever ‘Amazon Future Engineer Teacher of the Year Awards,’ Awarding Seven All-Star Teachers $25,000 Prize Packages for Exemplar Work with Students Across the Country",
         0,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "Samenvatting: Japanse NEDO en Panasonic behalen ‘s werelds hoogste conversie-efficiëntie van 16,09% voor grootste zonnecelmodule met Perovskite per gebied",
         0,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "Medical Properties Trust, Inc. Completes 2019 With Record $4.5 Billion in Acquisitions for 64% Growth Rate and Delivers Market-Leading Shareholder Returns",
         1,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "The stricken Bank of Jinzhou will unload $21 billion of assets to the central bank for less than a third of theirÃ‚Â r… https://t.co/xGuCP9lBlH",
         0,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "Some gamers accessing Google's new cloud gaming platform, Stadia, through a Chromecast Ultra dongle are reporting trouble… https://t.co/t6dCbMr0C3",
         2,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "“Vaping is taking us backward,” CVS Health CEO Larry Merlo says. “Something has to be done.” https://t.co/CwxbCtXvWW https://t.co/NBv10IejXx",
         2,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "“I think it’s a time of great transformation in retail right now.” @stitchfix CEO Katrina Lake discusses how the co… https://t.co/gQgzOBNaUg",
         0,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "Greenlane Renewables Signs New $7.0 Million System Supply Contract with the Renewable Natural Gas Company, a Leader in Landfill Gas to RNG Projects",
         0,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "Apple delaying the theatrical release of “The Banker” after one of the movie’s producers was accused of assault by… https://t.co/rqPyMuLZob",
         0,
         1.0
        ],
        [
         "2025-04-06T00:55:21.678+0000",
         "St. Louis Fed President Jim Bullard recommends declaring a  “National Pandemic Adjustment Period' and discusses thre… https://t.co/o942U7BKPI",
         0,
         1.0
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "tweet_time",
         "type": "\"timestamp\""
        },
        {
         "metadata": "{}",
         "name": "tweet",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sentiment",
         "type": "\"integer\""
        },
        {
         "metadata": "{\"ml_attr\":{\"type\":\"nominal\",\"num_vals\":3}}",
         "name": "prediction",
         "type": "\"double\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(predictions.select(\"tweet_time\", \"tweet\", \"sentiment\", \"prediction\").limit(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0a8f4b9b-5c5f-4bba-a5b2-aab54de7c632",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 2. Develop the Stream Processing Application\n",
    "1. Create a **structured streaming DataFrame** by reading from a folder where **new files** are added over time (`readStream`).\n",
    "2. using the same logic developed in the above\n",
    "3. save the `counts_df` to a memory table `counts`\n",
    "4. Don't start the streaming processing yet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1f47b8d4-4003-4ba5-8d8c-90f233b83bd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructType, StringType, IntegerType\n",
    "\n",
    "schema = StructType() \\\n",
    "    .add(\"tweet\", StringType()) \\\n",
    "    .add(\"sentiment\", IntegerType()) \\\n",
    "    .add(\"tweet_time\", StringType())\n",
    "\n",
    "streaming_df = spark.readStream.schema(schema).json(\"file:/databricks/driver/streaming_source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d432dd3-dde6-4ece-b681-9e4c16b72783",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "streaming_df_clean = streaming_df.drop(\"tweet_time\")\n",
    "\n",
    "streaming_predictions = pipeline_model.transform(streaming_df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "aa826357-f88f-476c-b571-793176391f78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "streaming_predictions_ts = streaming_predictions.withColumn(\n",
    "    \"tweet_time\", to_timestamp(\"tweet_time\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "08842d88-f7fe-4a07-b4f6-02dee3675d21",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import window, col\n",
    "\n",
    "counts_df = streaming_predictions_ts.groupBy(\n",
    "    window(col(\"tweet_time\"), \"1 minute\"),\n",
    "    col(\"prediction\")\n",
    ").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc112936-e876-4e12-9b73-40bf92972e77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "query = counts_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"memory\") \\\n",
    "    .queryName(\"counts\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "feb4c008-139e-4dc2-94ef-ec23ddab56d0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4. Fetch results from counts table and visualize it\n",
    "\n",
    " - fetch `prediction`, `time`, and `count` from `counts` table\n",
    "    - where `time` is based on the \"end\" of the window, and reformated using `date_format` with a format of `MMM-dd HH:mm\" \n",
    " - to prevent slow down, don't do any order by statements\n",
    " - choose an appropriate visualization to show the counts of different predictions in each window over time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b7236e82-e967-4697-b947-f4d1b6dc9d7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_handle = query.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f6b8e870-961a-4b03-9159-412de1110758",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Had to start the query or else I will get error in next code that counts table not found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05650b25-80c5-4c06-bd54-ecffaa7beba0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import date_format, col\n",
    "\n",
    "counts_query_df = spark.sql(\"SELECT * FROM counts\")\n",
    "\n",
    "formatted_counts_df = counts_query_df.select(\n",
    "    date_format(col(\"window.end\"), \"MMM-dd HH:mm\").alias(\"time\"),\n",
    "    col(\"prediction\").cast(\"int\").alias(\"sentiment_prediction\"),\n",
    "    col(\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "595dd1a3-e6d7-4cea-9289-e1be6f52df80",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>time</th><th>sentiment_prediction</th><th>count</th></tr></thead><tbody><tr><td>Apr-06 01:12</td><td>1</td><td>40</td></tr><tr><td>Apr-06 01:10</td><td>1</td><td>56</td></tr><tr><td>Apr-06 01:08</td><td>1</td><td>4393</td></tr><tr><td>Apr-06 01:09</td><td>1</td><td>65</td></tr><tr><td>Apr-06 01:11</td><td>1</td><td>64</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Apr-06 01:12",
         1,
         40
        ],
        [
         "Apr-06 01:10",
         1,
         56
        ],
        [
         "Apr-06 01:08",
         1,
         4393
        ],
        [
         "Apr-06 01:09",
         1,
         65
        ],
        [
         "Apr-06 01:11",
         1,
         64
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_prediction",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmb3JtYXR0ZWRfY291bnRzX2RmLCAxMCkK\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksViewfdf516d\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksViewfdf516d\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksViewfdf516d\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksViewfdf516d) SELECT `time`,SUM(`count`) `column_2fed817468`,`sentiment_prediction` FROM q GROUP BY `sentiment_prediction`,`time`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksViewfdf516d\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "sentiment_prediction",
             "id": "column_2fed817471"
            },
            "x": {
             "column": "time",
             "id": "column_2fed817470"
            },
            "y": [
             {
              "column": "count",
              "id": "column_2fed817468",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_2fed817468": {
             "name": "count",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "d58d0741-1395-49fc-a578-4f8b1a9c8b89",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 27.75,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "time",
           "type": "column"
          },
          {
           "column": "sentiment_prediction",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "time",
           "type": "column"
          },
          {
           "alias": "column_2fed817468",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "sentiment_prediction",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(formatted_counts_df, 10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "64715d95-2883-4b47-a07a-449f038f3dc6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 5. Start the streaming pipeline system\n",
    "\n",
    "- first start the data generator\n",
    "- then start the streaming processing app\n",
    "- then refresh the visualization from time to time to see the changes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c1b2c6ae-0daa-43ae-8589-0c71a418aeab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_handle = query.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8f1c559-e635-418a-8a55-f5418b9e7fc3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>time</th><th>sentiment_prediction</th><th>count</th></tr></thead><tbody><tr><td>Apr-06 01:14</td><td>1</td><td>59</td></tr><tr><td>Apr-06 01:12</td><td>1</td><td>52</td></tr><tr><td>Apr-06 01:15</td><td>1</td><td>62</td></tr><tr><td>Apr-06 01:13</td><td>1</td><td>62</td></tr><tr><td>Apr-06 01:10</td><td>1</td><td>56</td></tr><tr><td>Apr-06 01:08</td><td>1</td><td>4393</td></tr><tr><td>Apr-06 01:09</td><td>1</td><td>65</td></tr><tr><td>Apr-06 01:11</td><td>1</td><td>64</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         "Apr-06 01:14",
         1,
         59
        ],
        [
         "Apr-06 01:12",
         1,
         52
        ],
        [
         "Apr-06 01:15",
         1,
         62
        ],
        [
         "Apr-06 01:13",
         1,
         62
        ],
        [
         "Apr-06 01:10",
         1,
         56
        ],
        [
         "Apr-06 01:08",
         1,
         4393
        ],
        [
         "Apr-06 01:09",
         1,
         65
        ],
        [
         "Apr-06 01:11",
         1,
         64
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {},
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{}",
         "name": "time",
         "type": "\"string\""
        },
        {
         "metadata": "{}",
         "name": "sentiment_prediction",
         "type": "\"integer\""
        },
        {
         "metadata": "{}",
         "name": "count",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmb3JtYXR0ZWRfY291bnRzX2RmKQ==\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView37f0ef8\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView37f0ef8\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView37f0ef8\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView37f0ef8) SELECT `time`,SUM(`count`) `column_2fed817476`,`sentiment_prediction` FROM q GROUP BY `sentiment_prediction`,`time`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView37f0ef8\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "series": {
             "column": "sentiment_prediction",
             "id": "column_2fed817479"
            },
            "x": {
             "column": "time",
             "id": "column_2fed817478"
            },
            "y": [
             {
              "column": "count",
              "id": "column_2fed817476",
              "transform": "SUM"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "column",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_2fed817476": {
             "name": "count",
             "type": "column",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {},
       "nuid": "389b1506-28d7-4ecf-98cf-43f391dfb7ac",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 28.75,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "time",
           "type": "column"
          },
          {
           "column": "sentiment_prediction",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "time",
           "type": "column"
          },
          {
           "alias": "column_2fed817476",
           "args": [
            {
             "column": "count",
             "type": "column"
            }
           ],
           "function": "SUM",
           "type": "function"
          },
          {
           "column": "sentiment_prediction",
           "type": "column"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": [],
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(formatted_counts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4576dddb-175e-4b14-8f63-16d83298d326",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "### 6. Stop the data generator and the stream processing engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "696cfdb9-a47b-44e7-8a7f-57f6b0c4f1a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "stream_handle.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1e9798f3-8998-4ee4-9ced-0f993f58cdaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[55]: []"
     ]
    }
   ],
   "source": [
    "spark.streams.active"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1459092c-3632-438f-aeb1-cb46513716d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "All closed"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "1"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Stock_Predictor_Main",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}